approaches
- two main categories:
-- 1: one big fixed set of E-A axes, place all datasets onto it
---- would have to decide on the min., max., step size of both axes and stick to it
-- 2: normalise and scale each set individually
---- sets would have different resolutions
---- need to decide how many bins to have in each axis, and all have that


sets will need interpolation
- particularly as some sets may have a bunch of data clustered around a resonance, sparse otherwise


want to ideally set it up to easily do both approaches, and switch between them for comparison.
- need the model structure, training, etc. to be independent of exact data structure given
- the model is always just getting a big set of data where each has its own set of axes